#!/usr/bin/env python3
"""
Tests diagnostiques pour identifier les points d'am√©lioration du scoring RAG
Analyse d√©taill√©e des erreurs pour fine-tuning du syst√®me
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ollama import get_collection, enhance_medical_query, calculate_contextual_score, generate_imaging_recommendation_rag
from colorama import Fore, Style, init
import json

init(autoreset=True)

class RAGScoringDiagnostic:
    def __init__(self):
        self.collection = get_collection()
        self.errors = []
        self.scoring_issues = []
        
    def analyze_case(self, query, expected_motif, expected_status, case_name):
        """Analyse compl√®te d'un cas pour identifier les probl√®mes de scoring"""
        
        print(f"\n{Fore.CYAN}=== ANALYSE DIAGNOSTIQUE: {case_name} ==={Style.RESET_ALL}")
        print(f"Query: {query}")
        print(f"Attendu: motif='{expected_motif}', status='{expected_status}'")
        
        # 1. Enrichissement de la requ√™te
        enhanced_query = enhance_medical_query(query)
        print(f"\n{Fore.YELLOW}1. ENRICHISSEMENT:{Style.RESET_ALL}")
        print(f"   Original: {query}")
        print(f"   Enrichi:  {enhanced_query}")
        if enhanced_query != query:
            added = enhanced_query[len(query):].strip()
            print(f"   Ajout√©:   {Fore.GREEN}{added}{Style.RESET_ALL}")
        
        # 2. R√©cup√©ration des candidates
        results = self.collection.query(
            query_texts=[enhanced_query],
            n_results=10,
            include=['documents', 'metadatas', 'distances']
        )
        
        print(f"\n{Fore.YELLOW}2. TOP 10 CANDIDATES AVEC SCORING:{Style.RESET_ALL}")
        
        candidates = []
        for i, (doc, meta, dist) in enumerate(zip(results['documents'][0], results['metadatas'][0], results['distances'][0])):
            contextual_score = calculate_contextual_score(query, doc, meta, dist)
            motif = meta.get('motif', 'N/A')
            
            candidates.append({
                'rank': i+1,
                'motif': motif,
                'distance': dist,
                'contextual_score': contextual_score,
                'document': doc,
                'metadata': meta
            })
            
            # Identification du motif attendu
            is_expected = (motif == expected_motif)
            expected_marker = f"{Fore.GREEN}[ATTENDU]{Style.RESET_ALL}" if is_expected else ""
            
            print(f"   {i+1:2d}. Score: {contextual_score:.4f} | Dist: {dist:.4f} | {motif} {expected_marker}")
            print(f"       {doc[:80]}...")
        
        # 3. Analyse de la s√©lection
        candidates.sort(key=lambda x: x['contextual_score'], reverse=True)
        selected = candidates[0]
        
        print(f"\n{Fore.YELLOW}3. GUIDELINE S√âLECTIONN√âE:{Style.RESET_ALL}")
        print(f"   Motif: {selected['motif']}")
        print(f"   Score: {selected['contextual_score']:.4f}")
        print(f"   Distance: {selected['distance']:.4f}")
        print(f"   Contenu: {selected['document'][:100]}...")
        
        # 4. V√©rification si la bonne guideline est pr√©sente
        expected_guideline = None
        expected_rank = None
        for i, candidate in enumerate(candidates):
            if candidate['motif'] == expected_motif:
                expected_guideline = candidate
                expected_rank = i + 1
                break
        
        print(f"\n{Fore.YELLOW}4. ANALYSE DE L'ERREUR:{Style.RESET_ALL}")
        if expected_guideline:
            print(f"   ‚úÖ Guideline attendue TROUV√âE au rang {expected_rank}")
            print(f"   üìä Score attendu: {expected_guideline['contextual_score']:.4f}")
            print(f"   üìä Score s√©lectionn√©: {selected['contextual_score']:.4f}")
            
            if selected['motif'] != expected_motif:
                score_diff = selected['contextual_score'] - expected_guideline['contextual_score']
                print(f"   ‚ùå ERREUR DE SCORING: Diff√©rence de {score_diff:.4f}")
                
                # Analyse d√©taill√©e des scores
                self._analyze_scoring_difference(query, selected, expected_guideline, case_name)
        else:
            print(f"   ‚ùå Guideline attendue '{expected_motif}' NON TROUV√âE dans le top 10")
            print(f"   üîç Probl√®me de retrieval vectoriel (embeddings)")
        
        # 5. Test du r√©sultat final
        final_result = generate_imaging_recommendation_rag(query, self.collection)
        actual_status = self._extract_status(final_result)
        
        print(f"\n{Fore.YELLOW}5. R√âSULTAT FINAL:{Style.RESET_ALL}")
        print(f"   Attendu: {expected_status}")
        print(f"   Obtenu:  {actual_status}")
        
        success = (actual_status == expected_status)
        if success:
            print(f"   ‚úÖ {Fore.GREEN}SUCC√àS{Style.RESET_ALL}")
        else:
            print(f"   ‚ùå {Fore.RED}√âCHEC{Style.RESET_ALL}")
            self.errors.append({
                'case': case_name,
                'query': query,
                'expected_motif': expected_motif,
                'expected_status': expected_status,
                'actual_motif': selected['motif'],
                'actual_status': actual_status,
                'score_issue': selected['motif'] != expected_motif
            })
        
        return success
    
    def _analyze_scoring_difference(self, query, selected, expected, case_name):
        """Analyse d√©taill√©e des diff√©rences de scoring"""
        print(f"\n{Fore.MAGENTA}   ANALYSE D√âTAILL√âE DU SCORING:{Style.RESET_ALL}")
        
        # Recalcul des scores avec debug
        selected_score = calculate_contextual_score(query, selected['document'], selected['metadata'], selected['distance'])
        expected_score = calculate_contextual_score(query, expected['document'], expected['metadata'], expected['distance'])
        
        print(f"   üìä Score s√©lectionn√© ({selected['motif']}): {selected_score:.4f}")
        print(f"   üìä Score attendu ({expected['motif']}): {expected_score:.4f}")
        
        # Facteurs identifi√©s
        factors = []
        
        # Distance vectorielle
        if selected['distance'] < expected['distance']:
            factors.append(f"Distance vectorielle favorise s√©lectionn√© ({selected['distance']:.4f} < {expected['distance']:.4f})")
        else:
            factors.append(f"Distance vectorielle favorise attendu ({expected['distance']:.4f} < {selected['distance']:.4f})")
        
        # Analyse du texte
        query_lower = query.lower()
        selected_text_lower = selected['document'].lower()
        expected_text_lower = expected['document'].lower()
        
        # Correspondances de mots-cl√©s
        selected_matches = self._count_keyword_matches(query_lower, selected_text_lower)
        expected_matches = self._count_keyword_matches(query_lower, expected_text_lower)
        
        if selected_matches > expected_matches:
            factors.append(f"Plus de correspondances mots-cl√©s dans s√©lectionn√© ({selected_matches} vs {expected_matches})")
        elif expected_matches > selected_matches:
            factors.append(f"Plus de correspondances mots-cl√©s dans attendu ({expected_matches} vs {selected_matches})")
        
        for factor in factors:
            print(f"   ‚Ä¢ {factor}")
        
        # Recommandation d'am√©lioration
        self.scoring_issues.append({
            'case': case_name,
            'issue': f"Pr√©f√®re {selected['motif']} √† {expected['motif']}",
            'score_diff': selected_score - expected_score,
            'distance_diff': selected['distance'] - expected['distance'],
            'factors': factors
        })
    
    def _count_keyword_matches(self, query, text):
        """Compte les correspondances de mots-cl√©s"""
        query_words = set(query.split())
        text_words = set(text.split())
        return len(query_words.intersection(text_words))
    
    def _extract_status(self, result):
        """Extrait le statut du r√©sultat"""
        if 'RECOMMANDATION (RAG) : ' in result:
            status_part = result.split('RECOMMANDATION (RAG) : ')[1]
            if ' : ' in status_part:
                return status_part.split(' : ')[0]
        return 'AUTRE'
    
    def run_diagnostic_tests(self):
        """Lance les tests diagnostiques sur des cas probl√©matiques identifi√©s"""
        
        print(f"{Fore.CYAN}{'='*80}")
        print(f"üî¨ DIAGNOSTIC RAG - ANALYSE DES POINTS D'AM√âLIORATION")  
        print(f"{'='*80}{Style.RESET_ALL}")
        
        # Cas de test probl√©matiques identifi√©s
        test_cases = [
            {
                'name': 'Colique n√©phr√©tique vs Biliaire',
                'query': 'Homme 45 ans, douleur lombaire brutale irradiant vers l\'aine avec h√©maturie microscopique',
                'expected_motif': 'colique_nephretique',
                'expected_status': 'INDIQU√âE'
            },
            {
                'name': 'Appendicite adulte vs P√©diatrique',
                'query': 'Femme 28 ans, douleur fosse iliaque droite depuis 12h avec fi√®vre 38.5¬∞C et naus√©es',
                'expected_motif': 'douleur_abdominale',
                'expected_status': 'INDIQU√âE'
            },
            {
                'name': 'HTIC enfant correct',
                'query': 'Enfant 8 ans, vomissements matinaux r√©p√©t√©s depuis 1 semaine avec c√©phal√©es et troubles visuels',
                'expected_motif': 'pediatrie_neurologie',
                'expected_status': 'URGENTE'
            },
            {
                'name': 'M√©ningite vs HTIC',
                'query': 'Patient 34 ans, c√©phal√©es f√©briles brutales avec photophobie et raideur de nuque',
                'expected_motif': 'cephalees_aigues',
                'expected_status': 'URGENTE'
            },
            {
                'name': 'SEP vs autres neuro',
                'query': 'Femme 30 ans, troubles de la marche progressifs avec paresth√©sies des membres et fatigue',
                'expected_motif': 'sclerose_plaques',
                'expected_status': 'INDIQU√âE'
            },
            {
                'name': 'Grossesse - faux positif',
                'query': 'Patiente 22 ans, douleurs abdominales FID avec fi√®vre',
                'expected_motif': 'douleur_abdominale',
                'expected_status': 'INDIQU√âE'
            },
            {
                'name': 'Lombalgie commune - n√©gative',
                'query': 'Homme 35 ans, lombalgie commune depuis 4 semaines sans am√©lioration, bon √©tat g√©n√©ral',
                'expected_motif': 'lombalgie',
                'expected_status': 'AUCUNE'
            }
        ]
        
        successes = 0
        total = len(test_cases)
        
        for test_case in test_cases:
            success = self.analyze_case(
                test_case['query'],
                test_case['expected_motif'], 
                test_case['expected_status'],
                test_case['name']
            )
            if success:
                successes += 1
        
        self._generate_improvement_report(successes, total)
    
    def _generate_improvement_report(self, successes, total):
        """G√©n√®re un rapport d'am√©lioration d√©taill√©"""
        
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"üìä RAPPORT D'AM√âLIORATION - FINE-TUNING RECOMMENDATIONS")
        print(f"{'='*80}{Style.RESET_ALL}")
        
        print(f"\n{Fore.YELLOW}üìà PERFORMANCE GLOBALE:{Style.RESET_ALL}")
        accuracy = (successes / total) * 100
        print(f"   Pr√©cision: {accuracy:.1f}% ({successes}/{total})")
        
        if accuracy >= 80:
            print(f"   ‚úÖ {Fore.GREEN}Excellent - Fine-tuning mineur n√©cessaire{Style.RESET_ALL}")
        elif accuracy >= 60:
            print(f"   ‚ö†Ô∏è  {Fore.YELLOW}Correct - Am√©liorations cibl√©es recommand√©es{Style.RESET_ALL}")
        else:
            print(f"   ‚ùå {Fore.RED}Probl√©matique - R√©vision majeure n√©cessaire{Style.RESET_ALL}")
        
        print(f"\n{Fore.YELLOW}üîß POINTS D'AM√âLIORATION IDENTIFI√âS:{Style.RESET_ALL}")
        
        # Analyse des erreurs par type
        error_types = {}
        for error in self.errors:
            error_type = f"{error['expected_motif']} ‚Üí {error['actual_motif']}"
            if error_type not in error_types:
                error_types[error_type] = []
            error_types[error_type].append(error)
        
        for i, (error_type, errors) in enumerate(error_types.items(), 1):
            print(f"\n   {i}. {Fore.RED}ERREUR TYPE: {error_type}{Style.RESET_ALL}")
            print(f"      Fr√©quence: {len(errors)} cas")
            for error in errors:
                print(f"      ‚Ä¢ {error['case']}: {error['expected_status']} ‚Üí {error['actual_status']}")
        
        print(f"\n{Fore.YELLOW}üéØ RECOMMANDATIONS DE FINE-TUNING:{Style.RESET_ALL}")
        
        # Recommandations sp√©cifiques bas√©es sur l'analyse
        recommendations = []
        
        # Analyse des issues de scoring
        scoring_issues_by_type = {}
        for issue in self.scoring_issues:
            issue_type = issue['issue']
            if issue_type not in scoring_issues_by_type:
                scoring_issues_by_type[issue_type] = []
            scoring_issues_by_type[issue_type].append(issue)
        
        for i, (issue_type, issues) in enumerate(scoring_issues_by_type.items(), 1):
            avg_score_diff = sum(issue['score_diff'] for issue in issues) / len(issues)
            print(f"\n   {i}. {Fore.MAGENTA}AJUSTEMENT SCORING:{Style.RESET_ALL} {issue_type}")
            print(f"      Diff√©rence moyenne: {avg_score_diff:+.4f}")
            
            if avg_score_diff > 0.5:
                print(f"      ‚ö° Action: R√©duire le boost de {issue_type.split(' √† ')[0]}")
                recommendations.append(f"R√©duire bonification pour motif contenant '{issue_type.split(' √† ')[0]}'")
            elif avg_score_diff > 0.2:
                print(f"      üîß Action: Ajuster l√©g√®rement le scoring de {issue_type.split(' √† ')[0]}")
                recommendations.append(f"Ajustement mineur pour '{issue_type.split(' √† ')[0]}'")
            
            # Facteurs communs
            common_factors = {}
            for issue in issues:
                for factor in issue['factors']:
                    if factor not in common_factors:
                        common_factors[factor] = 0
                    common_factors[factor] += 1
            
            for factor, count in common_factors.items():
                if count > 1:
                    print(f"      üìä Facteur r√©current: {factor} ({count}x)")
        
        print(f"\n{Fore.GREEN}‚úÖ ACTIONS PRIORITAIRES:{Style.RESET_ALL}")
        for i, rec in enumerate(recommendations[:3], 1):  # Top 3 recommandations
            print(f"   {i}. {rec}")
        
        # M√©triques techniques
        print(f"\n{Fore.YELLOW}üìä M√âTRIQUES TECHNIQUES:{Style.RESET_ALL}")
        if self.scoring_issues:
            avg_score_diff = sum(abs(issue['score_diff']) for issue in self.scoring_issues) / len(self.scoring_issues)
            avg_distance_diff = sum(abs(issue['distance_diff']) for issue in self.scoring_issues) / len(self.scoring_issues)
            print(f"   Diff√©rence score moyenne: {avg_score_diff:.4f}")
            print(f"   Diff√©rence distance moyenne: {avg_distance_diff:.4f}")

def main():
    """Fonction principale"""
    try:
        diagnostic = RAGScoringDiagnostic()
        diagnostic.run_diagnostic_tests()
        return 0
    except Exception as e:
        print(f"\n‚ùå {Fore.RED}Erreur lors du diagnostic: {str(e)}{Style.RESET_ALL}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
